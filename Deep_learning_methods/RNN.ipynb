{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datasets import AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden=128, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.numlayers = num_layers\n",
    "        self.hidden = hidden\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.feat_dim, self.hidden, self.num_layers,batch_first=True)\n",
    "        self.linear1 = nn.Linear(self.hidden, 64)\n",
    "        self.linear2 = nn.Linear(64, 10)\n",
    "\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, length):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "            x: [batch, seq, feature]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, length, batch_first=True, enforce_sorted=False)\n",
    "        h0 = torch.randn(self.numlayers, batch_size, self.hidden).cuda()\n",
    "        c0 = torch.randn(self.numlayers, batch_size, self.hidden).cuda()\n",
    "        output, (hn, cn) = self.lstm(packed, (h0, c0))\n",
    "        y = self.linear1(hn[0, :, :])\n",
    "        y = self.linear2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to urban sound 8k\n",
    "data_root = \"/home/tiz007/228/228_data/UrbanSound8K/\"\n",
    "# path to label\n",
    "label_path = \"/home/tiz007/228/228_data/UrbanSound8K/metadata/UrbanSound8K.csv\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify mel_raw feature success\n"
     ]
    }
   ],
   "source": [
    "# initialize dataset (feature can be \"mfcc\", \"spec\", \"mel_raw\")\n",
    "audio_dataset = AudioDataset(3, DataRoot=data_root, LabelPath=label_path, feature=\"mel_raw\", mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lstm model\n",
    "lstm_model = LSTMModel(128)\n",
    "# to gpu\n",
    "lstm_model = lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloader\n",
    "data_loader = torch.utils.data.DataLoader(audio_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# lr\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch acc: 0.2234 avg loss: 0.0648\n",
      "epoch acc: 0.2699 avg loss: 0.0593\n",
      "epoch acc: 0.3269 avg loss: 0.0560\n",
      "epoch acc: 0.3507 avg loss: 0.0532\n",
      "epoch acc: 0.3699 avg loss: 0.0519\n",
      "epoch acc: 0.4145 avg loss: 0.0483\n",
      "epoch acc: 0.4639 avg loss: 0.0455\n",
      "epoch acc: 0.491 avg loss: 0.0434\n",
      "epoch acc: 0.5277 avg loss: 0.0412\n",
      "epoch acc: 0.5516 avg loss: 0.0396\n",
      "epoch acc: 0.559 avg loss: 0.0389\n",
      "epoch acc: 0.5913 avg loss: 0.0366\n",
      "epoch acc: 0.5914 avg loss: 0.0366\n",
      "epoch acc: 0.6171 avg loss: 0.0343\n",
      "epoch acc: 0.6408 avg loss: 0.0327\n",
      "epoch acc: 0.6512 avg loss: 0.0318\n",
      "epoch acc: 0.6577 avg loss: 0.0309\n",
      "epoch acc: 0.6759 avg loss: 0.0296\n",
      "epoch acc: 0.6291 avg loss: 0.0334\n",
      "epoch acc: 0.6515 avg loss: 0.0313\n",
      "epoch acc: 0.6839 avg loss: 0.0289\n",
      "epoch acc: 0.7012 avg loss: 0.0274\n",
      "epoch acc: 0.715 avg loss: 0.0266\n",
      "epoch acc: 0.717 avg loss: 0.0260\n",
      "epoch acc: 0.7296 avg loss: 0.0248\n",
      "epoch acc: 0.7368 avg loss: 0.0243\n",
      "epoch acc: 0.735 avg loss: 0.0244\n",
      "epoch acc: 0.7448 avg loss: 0.0234\n",
      "epoch acc: 0.7514 avg loss: 0.0227\n",
      "epoch acc: 0.7509 avg loss: 0.0229\n",
      "epoch acc: 0.7566 avg loss: 0.0219\n",
      "epoch acc: 0.7701 avg loss: 0.0212\n",
      "epoch acc: 0.7771 avg loss: 0.0205\n",
      "epoch acc: 0.7792 avg loss: 0.0196\n",
      "epoch acc: 0.7867 avg loss: 0.0195\n",
      "epoch acc: 0.7906 avg loss: 0.0192\n",
      "epoch acc: 0.802 avg loss: 0.0180\n",
      "epoch acc: 0.7974 avg loss: 0.0187\n",
      "epoch acc: 0.8049 avg loss: 0.0180\n",
      "epoch acc: 0.804 avg loss: 0.0176\n",
      "epoch acc: 0.8116 avg loss: 0.0176\n",
      "epoch acc: 0.8153 avg loss: 0.0167\n",
      "epoch acc: 0.8211 avg loss: 0.0161\n",
      "epoch acc: 0.8186 avg loss: 0.0165\n",
      "epoch acc: 0.8272 avg loss: 0.0159\n",
      "epoch acc: 0.8331 avg loss: 0.0150\n",
      "epoch acc: 0.8289 avg loss: 0.0156\n",
      "epoch acc: 0.8392 avg loss: 0.0146\n",
      "epoch acc: 0.8467 avg loss: 0.0143\n",
      "epoch acc: 0.8204 avg loss: 0.0165\n",
      "epoch acc: 0.8439 avg loss: 0.0138\n",
      "epoch acc: 0.8498 avg loss: 0.0136\n",
      "epoch acc: 0.8409 avg loss: 0.0147\n",
      "epoch acc: 0.8578 avg loss: 0.0128\n",
      "epoch acc: 0.8518 avg loss: 0.0134\n",
      "epoch acc: 0.8597 avg loss: 0.0126\n",
      "epoch acc: 0.8682 avg loss: 0.0119\n",
      "epoch acc: 0.8609 avg loss: 0.0126\n",
      "epoch acc: 0.8631 avg loss: 0.0123\n",
      "epoch acc: 0.8637 avg loss: 0.0123\n",
      "epoch acc: 0.8298 avg loss: 0.0155\n",
      "epoch acc: 0.8669 avg loss: 0.0118\n",
      "epoch acc: 0.8851 avg loss: 0.0104\n",
      "epoch acc: 0.8846 avg loss: 0.0104\n",
      "epoch acc: 0.8746 avg loss: 0.0112\n",
      "epoch acc: 0.8865 avg loss: 0.0103\n",
      "epoch acc: 0.8747 avg loss: 0.0113\n",
      "epoch acc: 0.8824 avg loss: 0.0105\n",
      "epoch acc: 0.8934 avg loss: 0.0097\n",
      "epoch acc: 0.8863 avg loss: 0.0102\n",
      "epoch acc: 0.8978 avg loss: 0.0093\n",
      "epoch acc: 0.9011 avg loss: 0.0087\n",
      "epoch acc: 0.9025 avg loss: 0.0089\n",
      "epoch acc: 0.8942 avg loss: 0.0093\n",
      "epoch acc: 0.8998 avg loss: 0.0088\n",
      "epoch acc: 0.8986 avg loss: 0.0089\n",
      "epoch acc: 0.9093 avg loss: 0.0080\n",
      "epoch acc: 0.8948 avg loss: 0.0095\n",
      "epoch acc: 0.7548 avg loss: 0.0245\n",
      "epoch acc: 0.781 avg loss: 0.0198\n",
      "epoch acc: 0.8459 avg loss: 0.0140\n",
      "epoch acc: 0.8843 avg loss: 0.0104\n",
      "epoch acc: 0.8888 avg loss: 0.0099\n",
      "epoch acc: 0.9059 avg loss: 0.0084\n",
      "epoch acc: 0.8943 avg loss: 0.0093\n",
      "epoch acc: 0.8965 avg loss: 0.0089\n",
      "epoch acc: 0.9038 avg loss: 0.0083\n",
      "epoch acc: 0.919 avg loss: 0.0074\n",
      "epoch acc: 0.8866 avg loss: 0.0101\n",
      "epoch acc: 0.9165 avg loss: 0.0075\n",
      "epoch acc: 0.9056 avg loss: 0.0086\n",
      "epoch acc: 0.9133 avg loss: 0.0076\n",
      "epoch acc: 0.9101 avg loss: 0.0079\n",
      "epoch acc: 0.9193 avg loss: 0.0070\n",
      "epoch acc: 0.9193 avg loss: 0.0072\n",
      "epoch acc: 0.9116 avg loss: 0.0076\n",
      "epoch acc: 0.9229 avg loss: 0.0069\n",
      "epoch acc: 0.9302 avg loss: 0.0062\n",
      "epoch acc: 0.9234 avg loss: 0.0068\n",
      "epoch acc: 0.9294 avg loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 100\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    # iterate through dataset\n",
    "   \n",
    "    # initialize epoch stat\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for idx, data in enumerate(data_loader):\n",
    "        \n",
    "        train_data, labels, length = data\n",
    "        \n",
    "        # data to gpu\n",
    "        train_data = train_data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        length = length.cuda()\n",
    "        \n",
    "        # normalize to [0,1]\n",
    "        #print(train_data.max(), train_data.min())\n",
    "        \n",
    "        train_data = train_data - train_data.min()\n",
    "        \n",
    "        prob = lstm_model(train_data, length)   \n",
    "        #print(prob, labels)\n",
    "        loss = loss_fn(prob, labels)\n",
    "        \n",
    "        output = prob.argmax(1)\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        correct_num += (output==labels).sum().double()\n",
    "        total_num += float(labels.shape[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    #print(loss.item())\n",
    "    #print(output, labels)\n",
    "\n",
    "    print(\"epoch acc: {:.4} avg loss: {:.4f}\".format(correct_num/total_num, loss_sum/total_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify mel_raw feature success\n",
      "epoch acc: 0.8934 avg loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "test_dataset = AudioDataset(3, DataRoot=data_root, LabelPath=label_path, feature=\"mel_raw\", mode=\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        test_data, labels, length = data\n",
    "        test_data = test_data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        length = length.cuda()\n",
    "\n",
    "\n",
    "        test_data = test_data - test_data.min()\n",
    "\n",
    "        prob = lstm_model(test_data, length)   \n",
    "        #print(prob, labels)\n",
    "        loss = loss_fn(prob, labels)\n",
    "\n",
    "        output = prob.argmax(1)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        correct_num += (output==labels).sum().double()\n",
    "        total_num += float(labels.shape[0])\n",
    "\n",
    "\n",
    "print(\"epoch acc: {:.4} avg loss: {:.4f}\".format(correct_num/total_num, loss_sum/total_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
